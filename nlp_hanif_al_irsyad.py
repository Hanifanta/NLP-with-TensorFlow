# -*- coding: utf-8 -*-
"""NLP-Hanif Al Irsyad

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YAqwTp0mqpYOpF9sD-465Mtr0BRbgS0w

**M-07 Pengembang Machine Learning dan Front-End**

Hanif Al Irsyad

Universitas Amikom Yogyakarta

Sleman,Yogyakarta
"""

import csv
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

!gdown --id 1rX10xeI3eUJmOLsc4pOPY6AnCLO8DxNj

df = pd.read_csv('/content/bbc-text.csv')
df.head()

df.category.value_counts()

category = pd.get_dummies(df.category)
df_new = pd.concat([df, category], axis=1)
df = df_new.drop(columns='category')
df.head(10)

df2 = df_new['text'].values
label = df_new[['business', 'entertainment', 'politics', 'sport', 'tech']].values

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(df2, label,test_size = 0.2,shuffle=True)

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

vocab_size = 5000
output_dim = 64
oov_token = "<OOV>"
max_len = 200
trunc_type = "post"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)
tokenizer.fit_on_texts(x_train)

index_kata = tokenizer.word_index

sekuens_latih = tokenizer.texts_to_sequences(x_train)
sekuens_test = tokenizer.texts_to_sequences(x_test)
padded_latih = pad_sequences(sekuens_latih, maxlen=max_len, truncating=trunc_type)
padded_test = pad_sequences(sekuens_test, maxlen=max_len, truncating=trunc_type)

print(padded_test.shape)

padded_latih

padded_test

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=output_dim, input_length=max_len),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
    ])

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

class Callbacknlp(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.85):
            print("\nReached %2.2f%% accuracy, training has been stop" %(logs.get('accuracy')*100))
            self.model.stop_training = True

callbacks = Callbacknlp()

num_epochs = 50

history = model.fit(padded_latih, y_train, epochs=num_epochs, validation_data=(padded_test, y_test), verbose=2, callbacks=[callbacks])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()